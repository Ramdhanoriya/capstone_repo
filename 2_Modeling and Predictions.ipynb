{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling and Predictions\n",
    "\n",
    "[Notebook 1: EDA and Cleaning](./1_EDA and Cleaning.ipynb)\n",
    "\n",
    "[Notebook 2: Modeling and Predictions](./2_Modeling and Predictions.ipynb)\n",
    "\n",
    "1. Features\n",
    "2. Resampling\n",
    "3. Modeling\n",
    "    - Scaling\n",
    "    - Lagged features\n",
    "    - Train test split, fit models, evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "\n",
    "import time\n",
    "\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from numpy import sqrt\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Dropout\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rcParams['figure.figsize'] = (13, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in pickle.\n",
    "data = pd.read_pickle('./pickle.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add hours feature\n",
    "data['Hour'] = data.index.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get features\n",
    "features = ['DOY', 'DNI', 'Air Temp', 'Hour', 'Humidity', 'Pressure', 'Wind Speed', 'Wind Dir']\n",
    "label = ['DNI']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resampling\n",
    "Resampling data to reduce runtime. Resampling during the period where the battery did not hold a charge, we see the  missing raw data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_re = data[features].resample('60T').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_re.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(data_re[data_re['DNI'].isnull()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_re.dropna(axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled = scaler.fit_transform(data_re)\n",
    "scaled = pd.DataFrame(scaled, index=data_re.index, columns=data_re.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lagged features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create df of all lagged features\n",
    "\n",
    "lags = [24, 72, 168]\n",
    "lstm_data = pd.DataFrame()\n",
    "lagged_feat_dict = {}\n",
    "\n",
    "for x in lags:\n",
    "    \n",
    "    # shift\n",
    "    lagged_data = scaled[features].shift(periods=x).dropna() # drop after looping\n",
    "    \n",
    "    # rename lagged features\n",
    "    lagged_col_names = {}\n",
    "    for y in lagged_data.columns:\n",
    "        lagged_col_names[y] = y + '_' + str(x)\n",
    "    lagged_data.rename(columns=lagged_col_names, inplace=True)\n",
    "    \n",
    "    # make dict of lagged_col_names (to reference for later)\n",
    "    lagged_feat_dict[x] = lagged_col_names.values()\n",
    "        \n",
    "    # make lstm_data the lagged data\n",
    "    lstm_data = pd.concat([lstm_data, lagged_data], axis=1)\n",
    "    \n",
    "# Add DNI target\n",
    "lstm_data = pd.concat([lstm_data, scaled[label]], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train test split, fit models, evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "year = 2012\n",
    "\n",
    "tts = len(lstm_data.loc[lstm_data.index.year < year])\n",
    "\n",
    "model_results = []\n",
    "\n",
    "epochs_ = 2\n",
    "batch_size_ = 1000\n",
    "dropout_ = .3\n",
    "\n",
    "for x in lags:\n",
    "    \n",
    "    print('Training w/ lag', x, 'hours:')\n",
    "    \n",
    "    feats = list(lagged_feat_dict[x])\n",
    "    \n",
    "    lstm_temp = lstm_data[feats + ['DNI']].dropna()\n",
    "    lstm_train = lstm_temp.loc[lstm_temp.index.year < year]\n",
    "    lstm_test = lstm_temp.loc[lstm_temp.index.year >= year]\n",
    "    \n",
    "    X_train = lstm_train[feats].values\n",
    "    X_test = lstm_test[feats].values\n",
    "    y_train = lstm_train['DNI'].values\n",
    "    y_test = lstm_test['DNI'].values\n",
    "    \n",
    "    X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "    X_test = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    "        \n",
    "    # design network\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(int(x), input_shape=(X_train.shape[1], X_train.shape[2]))) # num cells\n",
    "    model.add(Dropout(dropout_))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    \n",
    "    # fit network\n",
    "    history = model.fit(X_train, \n",
    "                        y_train, \n",
    "                        epochs=epochs_, \n",
    "                        batch_size=batch_size_, \n",
    "                        validation_data=(X_test, y_test), \n",
    "                        verbose=1, \n",
    "                        shuffle=False)\n",
    "    \n",
    "    # Save model for later\n",
    "    filename = str(int(time.time())) + '_model_' + str(x) + '_lag.h5'\n",
    "    model.save('./models/' + filename)\n",
    "    \n",
    "    # plot history\n",
    "    plt.plot(history.history['loss'], label='train')\n",
    "    plt.plot(history.history['val_loss'], label='test')\n",
    "    plt.legend()\n",
    "    plt.title('Training ' + str(x) + ' hours lag')\n",
    "    plt.show()\n",
    "\n",
    "    # make a prediction\n",
    "    yhat = model.predict(X_test)\n",
    "    \n",
    "    # Concat y_hat to dataframe for later\n",
    "    lstm_test = pd.concat([lstm_test, pd.DataFrame(yhat, columns=['yhat'], index=lstm_test.index)], axis=1)\n",
    "    \n",
    "    # Invert scaling (so we can get our forecast and evaluation in terms of DNI (W/m2))\n",
    "\n",
    "    # Get yhat and y_test\n",
    "    yhat = lstm_test['yhat']\n",
    "    original = lstm_test['DNI']\n",
    "\n",
    "    # Copy lstm_test\n",
    "    lstm_test_yhat = lstm_test.iloc[:, :8]\n",
    "    lstm_test_original = lstm_test.iloc[:, :8]\n",
    "\n",
    "    # Substitute yhat and y_test into lstm_test\n",
    "    lstm_test_yhat.iloc[:, 1] = yhat\n",
    "    lstm_test_original.iloc[:, 1] = original\n",
    "\n",
    "    # Inverse yhat\n",
    "    inv_yhat = scaler.inverse_transform(lstm_test_yhat)\n",
    "    inv_yhat = pd.DataFrame(inv_yhat).iloc[:, 1]\n",
    "\n",
    "    # Inverse y_test\n",
    "    inv_y_test = scaler.inverse_transform(lstm_test_original)\n",
    "    inv_y_test = pd.DataFrame(inv_y_test).iloc[:, 1]\n",
    "    \n",
    "    # Average RMSE, in terms of DNI (W/m2)\n",
    "    print('RMSE in DNI (W/m2): %.3f' % sqrt(mean_squared_error(inv_y_test, inv_yhat)))\n",
    "          \n",
    "    hours = 300\n",
    "\n",
    "    plt.plot(yhat[-hours:], label='yhat')\n",
    "    plt.plot(original[-hours:], label='y test')\n",
    "    plt.legend()\n",
    "    plt.title('Y test vs. y hat, ' + str(x) + ' hours lag')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # # calculate RMSE\n",
    "    rmse = sqrt(mean_squared_error(yhat, y_test))\n",
    "    r2 = r2_score(yhat, y_test)\n",
    "    print(str(x) + ' hours lag test R2 score: %.3f' % r2)\n",
    "    print(str(x) + ' hours lag test RMSE: %.3f' % rmse)\n",
    "    print()\n",
    "    \n",
    "    # Save all the results\n",
    "    \n",
    "    model_results_dict = {}\n",
    "    \n",
    "    model_results_dict['tts_year'] = year\n",
    "    model_results_dict['lag'] = x\n",
    "    model_results_dict['dropout'] = dropout_\n",
    "    model_results_dict['epochs'] = epochs_\n",
    "    model_results_dict['batch_size'] = batch_size_\n",
    "    model_results_dict['params'] = history.params\n",
    "    model_results_dict['loss'] = history.history\n",
    "    model_results_dict['rmse'] = rmse\n",
    "    model_results_dict['dni_rmse'] = sqrt(mean_squared_error(inv_y_test, inv_yhat))\n",
    "    model_results_dict['r2'] = r2\n",
    "    model_results_dict['model_filename'] = filename\n",
    "    model_results_dict['time_ran'] = int(time.time())\n",
    "    \n",
    "    model_results.append(model_results_dict)\n",
    "    \n",
    "# Reads in old results and concats with new results\n",
    "\n",
    "new_res_df = pd.DataFrame(model_results)\n",
    "old_res_df = pd.read_csv('./results/results.csv', index_col=0)\n",
    "res_df = pd.concat([old_res_df, new_res_df], axis=0, sort=False).reset_index(drop=True)\n",
    "res_df.to_csv('./results/results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
